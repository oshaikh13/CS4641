{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load the modules\n",
    "\n",
    "from glob import glob \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras,cv2,os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tqdm import tqdm_notebook,trange\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import gc #garbage collection, we need to save all the RAM we can\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "077d54fd1297b6707ea5f202ec706085599225c6"
   },
   "source": [
    "# 3. Loading the data\n",
    "We'll start by creating a pandas data frame containing the path of all the files in the `train_path` folder and then read the matching labels from the provided csv file.\n",
    "\n",
    "## Load the labels and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d72f6d8131277d5a1452d92677bcaaae8117165b"
   },
   "outputs": [],
   "source": [
    "#set paths to training and test data\n",
    "path = '..\\data\\PCam\\\\' #adapt this path, when running locally\n",
    "train_path = path + 'train\\\\'\n",
    "test_path = path + 'test\\\\'\n",
    "\n",
    "df = pd.DataFrame({'path': glob(os.path.join(train_path,'*.tif'))}) # load the filenames\n",
    "# print(df)\n",
    "df['id'] = df.path.map(lambda x: x.split('\\\\')[3].split(\".\")[0]) # keep only the file names in 'id'\n",
    "labels = pd.read_csv(path+\"train_labels.csv\") # read the provided labels\n",
    "df = df.merge(labels, on = \"id\") # merge labels and filepaths\n",
    "df.head(3) # print the first three entrys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e32d37b60a88fb0c038ab2a6938bd46198e79740"
   },
   "source": [
    "## Load the images\n",
    "\n",
    "Now, we will load some of the images. As [interactive kernels with GPU](https://www.kaggle.com/docs/kernels#the-kernels-environment) currently offer about 14 GB of RAM, we will take care to keep the images in the uint8 format (i.e. pixel values are integers between 0 and 255) to reduce the memory footprint. Processing of the images often requires converting them to float32, which would require additional space.\n",
    "\n",
    "We'll declare a function to load a set number of images and then load 10000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5087ee421fbaf96cbd18df1467cf14c34b4f7289"
   },
   "outputs": [],
   "source": [
    "def load_data(N,df):\n",
    "    \"\"\" This functions loads N images using the data df\n",
    "    \"\"\"\n",
    "    # allocate a numpy array for the images (N, 96x96px, 3 channels, values 0 - 255)\n",
    "    X = np.zeros([N,96,96,3],dtype=np.uint8) \n",
    "    #convert the labels to a numpy array too\n",
    "    y = np.squeeze(df.as_matrix(columns=['label']))[0:N]\n",
    "    #read images one by one, tdqm notebook displays a progress bar\n",
    "    for i, row in tqdm_notebook(df.iterrows(), total=N):\n",
    "        if i == N:\n",
    "            break\n",
    "        X[i] = cv2.imread(row['path'])\n",
    "          \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3bd89f6d9e00472c46b1c9c617e3d090c9fccce"
   },
   "source": [
    "# Setup the model\n",
    "We will now focus on creating a simple model for this problem. This is usually the point, where you would want to start considering our previous conclusions, but to keep it simple, we will assume, we did not draw any meaningful conclusions. As the data is - in comparison to other Kaggle challenges - relatively well balanced and accesible, this should be ok. You can use the insight to craft a better model later!\n",
    "\n",
    "Let's start by loading all the data, not just a subset as before. This will likely require a few minutes. However, we only need to do it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30e69a2a034e279da6eeb0fc09f1a58b58fc0401"
   },
   "outputs": [],
   "source": [
    "N = df[\"path\"].size # get the number of images in the training data set\n",
    "X,y = load_data(N=N,df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e5b2386785b29f3a332daffa0160d14aa3a562e"
   },
   "source": [
    "We will use the garbage collector and unbind some variables to free up space in our RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e14b07edae9d62cc873b1b10c5288c72bfbf9671"
   },
   "outputs": [],
   "source": [
    "#Collect garbage\n",
    "positives_samples = None\n",
    "negative_samples = None\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c56525b3ee4c5f65255422ec035c057d350de292"
   },
   "source": [
    "Now, we will split the data into a [training and validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets). Due to the RAM limitations, we will just do this in-place by specifying an index at which we will split. We'll use 80% of the data for training and 20% to validate that our model can generalize to new data. After that, to avoid any influence of a possible previous sorting of data we will shuffle the data (in-place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60644d18d00811a12f6270ac42f8c745902018a8"
   },
   "outputs": [],
   "source": [
    "training_portion = 0.9 # Specify training/validation ratio\n",
    "split_idx = int(np.round(training_portion * y.shape[0])) #Compute split idx\n",
    "\n",
    "np.random.seed(420) #set the seed to ensure reproducibility\n",
    "\n",
    "#shuffle\n",
    "idx = np.arange(y.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "X_train = X[:split_idx]\n",
    "y_train = y[:split_idx]\n",
    "X_test = X[split_idx:]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f6bf258977d100a6781f13e9dfe31009fec0bb3"
   },
   "source": [
    "Let's declare our neural network architecture now. This kernel uses [keras](https://keras.io/), which makes it very easy to setup a neural network and start training it.\n",
    "\n",
    "The model architecture is taken from [another kernel](https://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-10min-0-925-lb). It is a relatively simple convolutional neural network with three blocks of [convolutional layers, batch normalization, pooling and dropout](https://cs231n.github.io/convolutional-networks/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9434b86b47f6132843b328d4fbf326aad01b24c0"
   },
   "outputs": [],
   "source": [
    "#just some network parameters, see above link regarding the layers for details\n",
    "def batchAfter():\n",
    "    kernel_size = (3,3)\n",
    "    pool_size= (2,2)\n",
    "    first_filters = 32\n",
    "    second_filters = 64\n",
    "    third_filters = 128\n",
    "\n",
    "    dropout_conv = 0.3\n",
    "    dropout_dense = 0.5\n",
    "\n",
    "    #initialize the model\n",
    "    model = Sequential()\n",
    "\n",
    "    #now add layers to it\n",
    "\n",
    "    #conv block 1\n",
    "    model.add(Conv2D(first_filters, kernel_size, input_shape = (96, 96, 3)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(first_filters, kernel_size, use_bias=False))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size = pool_size)) \n",
    "    model.add(Dropout(dropout_conv))\n",
    "\n",
    "    #conv block 2\n",
    "    model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size = pool_size))\n",
    "    model.add(Dropout(dropout_conv))\n",
    "\n",
    "    #conv block 3\n",
    "    model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size = pool_size))\n",
    "    model.add(Dropout(dropout_conv))\n",
    "\n",
    "    #a fully connected (also called dense) layer at the end\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, use_bias=False))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_dense))\n",
    "\n",
    "\n",
    "    # sigmoid\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(0.001), \n",
    "              metrics=['accuracy'])\n",
    "    return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just some network parameters, see above link regarding the layers for details\n",
    "def batchBefore():\n",
    "    kernel_size = (3,3)\n",
    "    pool_size= (2,2)\n",
    "    first_filters = 32\n",
    "    second_filters = 64\n",
    "    third_filters = 128\n",
    "\n",
    "    dropout_conv = 0.3\n",
    "    dropout_dense = 0.5\n",
    "\n",
    "    #initialize the model\n",
    "    model = Sequential()\n",
    "\n",
    "    #now add layers to it\n",
    "\n",
    "    #conv block 1\n",
    "    model.add(Conv2D(first_filters, kernel_size, input_shape = (96, 96, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(first_filters, kernel_size, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size = pool_size)) \n",
    "    model.add(Dropout(dropout_conv))\n",
    "\n",
    "    #conv block 2\n",
    "    model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size = pool_size))\n",
    "    model.add(Dropout(dropout_conv))\n",
    "\n",
    "    #conv block 3\n",
    "    model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size = pool_size))\n",
    "    model.add(Dropout(dropout_conv))\n",
    "\n",
    "    #a fully connected (also called dense) layer at the end\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(dropout_dense))\n",
    "\n",
    "\n",
    "    # sigmoid\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(0.001), \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "859fd386ac84192fe162ecb33094e586e1a9d6bf"
   },
   "source": [
    "To start training with keras we need to [compile](https://keras.io/models/model/#compile) our model. We will use a batch size of 50, i.e., feed the network 50 images at once. Further, we will use [binary crossentropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) as loss function and the [Adam optimizer](ruder.io/optimizing-gradient-descent/index.html). We set the [learning rate](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10) of 0.001 for now.\n",
    "\n",
    "As output we will get the classification accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b88d166534707fac6a1553adc0b0b94f5a6baf76"
   },
   "source": [
    "We are now ready to start training our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c23406dd90b311f7683e6d5b76a5c929d8d52bc4"
   },
   "source": [
    "# 6. Train and validate the model\n",
    "We will now train the model for three epochs (should take ~20mins). That means the model will have performed a [forward and backward pass](https://cs231n.github.io/optimization-2/) for each image in the training exactly three times.\n",
    "\n",
    "To do so, we will split the training data in batches and feed one batch after another into the network. [The batch size is a critical parameter for training a neural network](https://cs231n.github.io/neural-networks-3/#baby). \n",
    "\n",
    "Keras can do the splitting automatically for you, but, I thought, this way it is more transparent what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd873cb97936e734aca949c2dc7673b53cbcaa8d"
   },
   "source": [
    "Now, to verify that our model also works with data it hasn't seen yet, we will perform a validation epoch, i.e., check the accuracy on the validation set without further training the network. This is achieved using the [`test_on_batch` function](https://keras.io/models/sequential/#test_on_batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "histories = [];\n",
    "num_epochs = 5\n",
    "num_batches = 32\n",
    "k_fold_results = {}\n",
    "for j in range(2):\n",
    "    \n",
    "    for i in range(k):\n",
    "        model = None\n",
    "        gc.collect();\n",
    "        print();\n",
    "        if j == 0:\n",
    "            print(\"TESTING BATCH-NORM BEFORE ON K-FOLD: \" + str(i))\n",
    "            model = batchBefore();\n",
    "        else:\n",
    "            print(\"TESTING BATCH-NORM AFTER ON K-FOLD: \" + str(i))\n",
    "            model = batchAfter();\n",
    "            \n",
    "        val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        partial_train_data = np.concatenate(\n",
    "            [X_train[:i * num_val_samples],\n",
    "            X_train[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [y_train[:i * num_val_samples],\n",
    "            y_train[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "        history = model.fit(partial_train_data, partial_train_targets,\n",
    "                            epochs=5, batch_size=num_batches, \n",
    "                            verbose=2, callbacks=[TQDMNotebookCallback()],\n",
    "                            validation_data=(val_data, val_targets))\n",
    "        \n",
    "        if not j in k_fold_results:\n",
    "            k_fold_results[j] = [];\n",
    "            \n",
    "        k_fold_results[j].append(history);\n",
    "        print();\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4726acfd86e61d1ad1c65fa0793204c2b128aafc"
   },
   "source": [
    "# 7. Create a submission\n",
    "Well, now that we have a trained a model, we can create a submission by predicting the labels of the test data and see, where we are at in the leaderboards!\n",
    "\n",
    "Let's just first clear up some RAM. The creation of the submission is a modified version of the on presented in [this kernel](https://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-10min-0-925-lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3217bef913dde489dd23aebceaea82a4e88a1748"
   },
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d5d71c896efb4018434d00d1aab03d3355cb367"
   },
   "outputs": [],
   "source": [
    "means = []\n",
    "\n",
    "for hist_idx in k_fold_results:\n",
    "    \n",
    "    mean = {\n",
    "        'val_loss':[0] * 5,\n",
    "        'val_acc':[0] * 5,\n",
    "        'loss':[0] * 5,\n",
    "        'acc':[0] * 5\n",
    "    }\n",
    "    \n",
    "    counter = 0;\n",
    "    for hist_obj in k_fold_results[hist_idx]:\n",
    "        for mean_key in hist_obj.history:\n",
    "            for j in range(0, len(hist_obj.history[mean_key])):\n",
    "                mean[mean_key][j] += hist_obj.history[mean_key][j]\n",
    "            \n",
    "    print(counter)\n",
    "    for key in mean.keys():\n",
    "        mean[key] = [x / 4 for x in mean[key]]\n",
    "\n",
    "    means.append(mean)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6113e9f267e0902a423490b75fa331f524e2aae"
   },
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "historyBatchBefore = means[0]\n",
    "plt.figure(figsize=(9,5))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(historyBatchBefore['acc'])\n",
    "plt.plot(historyBatchBefore['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "historyBatchAfter = means[1]\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(historyBatchAfter['acc'])\n",
    "plt.plot(historyBatchAfter['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [];\n",
    "ys = [];\n",
    "times = [];\n",
    "for i in range(1, 11):\n",
    "    gc.collect();\n",
    "    y_train_subset = y_train[:int((len(X_train)/10) * i)]\n",
    "    X_train_subset = X_train[:int((len(X_train)/1091) * i)]\n",
    "    model = batchBefore()\n",
    "    start_time = time.time();\n",
    "    model.fit(X_train_subset, y_train_subset,epochs=5, batch_size=32, callbacks=[TQDMNotebookCallback()], verbose=0)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "    xs.append(i * 10);\n",
    "    ys.append(score)\n",
    "    times.append(time.time() - start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = batchBefore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "ys = [[0.3375332970108676, 0.8564286688178885],\n",
    " [0.3293629326703429, 0.8648366131891105],\n",
    " [0.5694290703417445, 0.7891651138481116],\n",
    " [0.7749491917055851, 0.7836658637458528],\n",
    " [0.336175938627912, 0.8522928691406616],\n",
    " [0.994924304453139, 0.7673044585018967],\n",
    " [0.2400954164402412, 0.9011498432221621],\n",
    " [0.22097328733323526, 0.910375857824023],\n",
    " [0.2286083311565799, 0.9059219197436353],\n",
    " [0.7929028012517072, 0.9043582939958323]]\n",
    "\n",
    "# taken from PCam remaining classifiers\n",
    "decisionYs = [0.5134435316436542, 0.5745254325436161, \n",
    "              0.5453134543164626, 0.5854262654274613, \n",
    "              0.5954623654275464, 0.5464353254637411, \n",
    "              0.6016344253768487, 0.6363447587598674, \n",
    "              0.6612697386754643, 0.6745365375395623]\n",
    "boostingY = [0.49983219895391852, 0.5138847390187585, \n",
    "              0.5005498029588590, 0.5183705841036052, \n",
    "              0.5231069410663139, 0.5443301930503013, \n",
    "              0.5743598338850385, 0.6112378434157855, \n",
    "              0.6612697386754643, 0.6564223157753243]\n",
    "kNNY = [0.4374819840624884, 0.4934100239593562, \n",
    "        0.5045431587048912, 0.5001020394006202, \n",
    "        0.5331990401825637, 0.4489520812395820, \n",
    "        0.5182939050123983, 0.5313755816002983, \n",
    "        0.4982370304672290, 0.5448900128395810]\n",
    "SVMy = [0.47447201884504075, 0.5046718914746082, \n",
    "        0.5016246562546889, 0.4582748666459793, \n",
    "        0.4477714602915569, 0.5041805276804852, \n",
    "        0.4682776883697699, 0.44208344224584517, \n",
    "        0.49422385876121694, 0.4995442189399771]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newYs = []\n",
    "for elem in ys:\n",
    "    newYs.append(elem[1])\n",
    "print(newYs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, newYs)\n",
    "plt.plot(xs, decisionYs)\n",
    "plt.plot(xs, boostingY)\n",
    "plt.plot(xs, kNNY)\n",
    "plt.plot(xs, SVMy)\n",
    "plt.xlabel(\"Percentage of Training Dataset\")\n",
    "plt.ylabel(\"Testing Accuracy\")\n",
    "plt.legend([\"Neural Network\", \"Decision Tree\", \"Boosting\", \"kNN\", \"SVM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
